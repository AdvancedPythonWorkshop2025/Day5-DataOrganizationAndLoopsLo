{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install h5py numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Mixed-Type, Labelled, Data and Metadata to HDF5 Files using `h5py`\n",
    "\n",
    "HDF5 files have a lot more features:\n",
    "  - They are highly cross-platform and work with a wide variety of tools\n",
    "  - They can store many different datasets in a single file (or even in multiple linked files)\n",
    "  - They can store metadata alonside the data\n",
    "  - They let you store data hierarchically, making a nice dict-like nested organization for your data\n",
    "  - They can compress your data.\n",
    "  - They let you work with data that is larger than memory, making it easy to read in only the data that you need.\n",
    "  - They can be easily previewed and inspected using the https://myhdf5.hdfgroup.org/ web tool!\n",
    "  \n",
    "So many features!  Here, we'll get a basic senses of how they work by using the `h5py` library, which gives us a dict-like, Numpy-native interface to HDF5 files and is used internally by many popular Python frameworks.\n",
    "\n",
    "\n",
    "\n",
    "| Code | Description |\n",
    "| :-- | :-- |\n",
    "| **`f = h5py.File('filename.h5', 'w')`** | Open an h5py file object for writing |\n",
    "| **`f.close()`** | Closes the h5py file and releases the linked file back to the operating system. |\n",
    "| **`f.create_dataset('temp', data=x)`** | Write an array called 'temp' with the data in the numy array `x` into the HDF5 file |\n",
    "| **`f.create_dataset('data/temp', data=x)`** | Write an array called 'temp' in the folder called \"data\" with the data in the numy array `x` into the HDF5 file |\n",
    "| **`f.attrs['name'] = 'Session 1'`** | Set an attribute as metadata onto the root group of the HDF5 file -- this works like a normal Python dictionary |\n",
    "| **`f['x'].attrs['id'] = 'ABC'`** | Set an attribute as metadata onto the 'x' node of the HDF file |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Arrays in HDF5 as \"Datasets\" and Organizing them in \"Groups\"\n",
    "\n",
    "| Code | Description |\n",
    "| :-- | :-- |\n",
    "| **`f = h5py.File('filename.h5', 'w')`** | Open an h5py file object for writing |\n",
    "| **`f.close()`** | Closes the h5py file and releases the linked file back to the operating system. |\n",
    "| **`f['x'] = np.array([1, 2, 3])`** | Put a Numpy array into a **dataset** called `x` |\n",
    "| **`f['folder/x'] = np.array([1, 2, 3])`** | Put a Numpy array into  **dataset** called `x`, in a **group** called `folder` |\n",
    "| **`f.create_dataset('folder/x', shape=(100,2), dtype=np.uint8)`** | Make an empty dataset in the file |\n",
    "| **`f.create_dataset('folder/x', data=my_array, compression='gzip')`** | Store a numpy array as a dataset using a given compression algorithm. |\n",
    "| **`f['x'][:]`** | Read in the dataset into a Numpy array. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Open and Close a File**.  Open an HDF5 file named `exercise1.h5` for storing EEG data, then close it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise: Store a Small Array of Spike Counts**. Store a small 1D NumPy array of spike counts in a file called `exercise2.h5`, in a dataset called `spike_counts`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Read Back the Spike Counts**.  Read the `spike_counts` dataset from `exercise2.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Create a Dataset Inside a Folder for LFP**. Create a dataset named `lfp_data` inside a group called `session1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5: Read the LFP Dataset from the Folder**.  Read the `lfp_data` from the `session1` group in `exercise4.h5`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Create an Empty Dataset for Neuron Responses**. Create an empty dataset called `neuron_responses` with shape (100, 2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Create and Immediately Fill an Empty Dataset for Trials**. Create an empty dataset called `trials` and fill it with random integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Compressed Dataset for EEG**.  Create a compressed dataset called `eeg_data` using gzip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Read the Compressed EEG Data**. Read the `eeg_data` dataset from the previous exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Store a 2D Array of Spike Trains in a Group**  Store a 2D array of spike trains in a group called `recordings`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Read the Spike Trains**.  Read the `spike_trains` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Put Multiple Datasets into a Schema**:  Create a file with the following organization:\n",
    "\n",
    "  - behavior/\n",
    "    - trial: 10 values, uint8\n",
    "    - time: 100 values,  float32 \n",
    "    - lick_rate: 10 x 100 values, float32\n",
    "  - ephys/\n",
    "    - spike_times: 50 values, float64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Metadata in HDF5 as \"Attributes\"\n",
    "\n",
    "Labeling our data and keeping those labels together with the data itself is a key practice in having well-organized data, and hdf5 makes it easy to put labels directly inside the files.  \n",
    "\n",
    "One practice to be aware of:  \n",
    "  - General metadata for the file is often attached to the \"root\" group, so it's easy to find.  Things like experiment-level or session-level metadata are often stored there.  \n",
    "  - Metadata that describes a specific array are often attached to the hdf5 dataset itself.  Things like dimension labels, units, text descriptions of what the data represents can usually be found here.\n",
    "\n",
    ".\n",
    "\n",
    "| Code | Description |\n",
    "| :-- | :-- |\n",
    "| **`f.attrs['subject'] = 'Doug'`** | Add a \"subject\" attribute to the root-level group. |\n",
    "| **`f['x'].attrs['subject'] = 'Doug'`** | Add a \"subject\" attribute to the 'x' group or dataset. |\n",
    "| **`dict(f.attrs)`** | Get all the attributes attached to the root-level group as a dict.\n",
    "| **`dict(f['x'].attrs)`** | Get all the attributes attached to the 'x' group or dataset as a dict.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Create a file with the following schema:\n",
    "\n",
    "```\n",
    "meta_ex1.h5\n",
    "└─ / (root)\n",
    "   └─ subject_id = \"RatA\"  (attribute)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Reopen `meta_ex1.h5` and get the \"subject_id\" attribute value from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Create a file with the following schema:\n",
    "```\n",
    "meta_ex2.h5\n",
    "└─ / (root)\n",
    "   └─ lfp_data (dataset, shape=(100,))\n",
    "      └─ recording_date = \"2025-03-31\"  (attribute)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Reopen `meta_ex2.h5` and the `recording_date` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Create a file with the following schema:\n",
    "\n",
    "```\n",
    "meta_ex3.h5\n",
    "└─ / (root)\n",
    "   ├─ experimenter = \"Dr. Gray\"       (attribute)\n",
    "   ├─ experiment_type = \"Optogenetics\" (attribute)\n",
    "   └─ lab = \"NeuroLab\"                (attribute)\n",
    "   └─ spike_trains (dataset, shape=(4,50))\n",
    "      ├─ brain_region = \"Hippocampus\"   (attribute)\n",
    "      └─ num_channels = 4              (attribute)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Reopen `meta_ex3.h5` and get all the root-level metadata into a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling Dimensions of Datasets by Linking Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
