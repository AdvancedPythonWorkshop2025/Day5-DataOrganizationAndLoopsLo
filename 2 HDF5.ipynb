{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install h5py numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Mixed-Type, Labelled, Data and Metadata to HDF5 Files using `h5py`\n",
    "\n",
    "HDF5 files have a lot more features:\n",
    "  - They are highly cross-platform and work with a wide variety of tools\n",
    "  - They can store many different datasets in a single file (or even in multiple linked files)\n",
    "  - They can store metadata alonside the data\n",
    "  - They let you store data hierarchically, making a nice dict-like nested organization for your data\n",
    "  - They can compress your data.\n",
    "  - They let you work with data that is larger than memory, making it easy to read in only the data that you need.\n",
    "  - They can be easily previewed and inspected using the https://myhdf5.hdfgroup.org/ web tool!\n",
    "  \n",
    "So many features!  Here, we'll get a basic senses of how they work by using the `h5py` library, which gives us a dict-like, Numpy-native interface to HDF5 files and is used internally by many popular Python frameworks.\n",
    "\n",
    "\n",
    "\n",
    "| Code | Description |\n",
    "| :-- | :-- |\n",
    "| **`f = h5py.File('filename.h5', 'w')`** | Open an h5py file object for writing |\n",
    "| **`f.close()`** | Closes the h5py file and releases the linked file back to the operating system. |\n",
    "| **`f.create_dataset('temp', data=x)`** | Write an array called 'temp' with the data in the numy array `x` into the HDF5 file |\n",
    "| **`f.create_dataset('data/temp', data=x)`** | Write an array called 'temp' in the folder called \"data\" with the data in the numy array `x` into the HDF5 file |\n",
    "| **`f.attrs['name'] = 'Session 1'`** | Set an attribute as metadata onto the root group of the HDF5 file -- this works like a normal Python dictionary |\n",
    "| **`f['x'].attrs['id'] = 'ABC'`** | Set an attribute as metadata onto the 'x' node of the HDF file |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Arrays in HDF5 as \"Datasets\" and Organizing them in \"Groups\"\n",
    "\n",
    "| Code | Description |\n",
    "| :-- | :-- |\n",
    "| **`f = h5py.File('filename.h5', 'w')`** | Open an h5py file object for writing |\n",
    "| **`f.close()`** | Closes the h5py file and releases the linked file back to the operating system. |\n",
    "| **`f['x'] = np.array([1, 2, 3])`** | Put a Numpy array into a **dataset** called `x` |\n",
    "| **`f['folder/x'] = np.array([1, 2, 3])`** | Put a Numpy array into  **dataset** called `x`, in a **group** called `folder` |\n",
    "| **`f.create_dataset('folder/x', shape=(100,2), dtype=np.uint8)`** | Make an empty dataset in the file |\n",
    "| **`f.create_dataset('folder/x', data=my_array, compression='gzip')`** | Store a numpy array as a dataset using a given compression algorithm. |\n",
    "| **`f['x'][:]`** | Read in the dataset into a Numpy array. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Open and Close a File**.  Open an HDF5 file named `exercise1.h5` for storing EEG data, then close it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('exercise1.h5', 'w')\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise: Store a Small Array of Spike Counts**. Store a small 1D NumPy array of spike counts in a file called `exercise2.h5`, in a dataset called `spike_counts`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('exercise2.h5', 'w')\n",
    "f['spike_counts'] = np.array([5, 2, 7, 3])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Read Back the Spike Counts**.  Read the `spike_counts` dataset from `exercise2.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 2, 7, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = h5py.File('exercise2.h5', 'r')\n",
    "data_read = f['spike_counts'][:]\n",
    "f.close()\n",
    "data_read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Create a Dataset Inside a Folder for LFP**. Create a dataset named `lfp_data` inside a group called `session1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('exercise4.h5', 'w')\n",
    "f['session1/lfp_data'] = np.array([0.1, 0.5, 0.2, 0.9])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5: Read the LFP Dataset from the Folder**.  Read the `lfp_data` from the `session1` group in `exercise4.h5`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.5, 0.2, 0.9])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = h5py.File('exercise4.h5', 'r')\n",
    "lfp_values = f['session1/lfp_data'][:]\n",
    "f.close()\n",
    "lfp_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Create an Empty Dataset for Neuron Responses**. Create an empty dataset called `neuron_responses` with shape (100, 2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('exercise6.h5', 'w')\n",
    "f.create_dataset('neuron_responses', shape=(100, 2), dtype=np.uint8)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Create and Immediately Fill an Empty Dataset for Trials**. Create an empty dataset called `trials` and fill it with random integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('exercise7.h5', 'w')\n",
    "f.create_dataset('trials', shape=(10, 3), dtype=np.uint8)\n",
    "f['trials'][:] = np.random.randint(0, 256, (10, 3), dtype=np.uint8)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Compressed Dataset for EEG**.  Create a compressed dataset called `eeg_data` using gzip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(128, 256)\n",
    "f = h5py.File('exercise8.h5', 'w')\n",
    "f.create_dataset('eeg_data', data=data, compression='gzip')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Read the Compressed EEG Data**. Read the `eeg_data` dataset from the previous exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.22045578, -0.35791492, -0.81303986, ...,  0.55139236,\n",
       "        -0.48164113, -0.57486016],\n",
       "       [ 1.11067651,  0.09371266, -2.18472438, ..., -0.37044065,\n",
       "         1.19816675, -0.65564871],\n",
       "       [-0.45968531,  1.29586459,  0.02311049, ..., -1.2418498 ,\n",
       "        -0.25576863, -0.19520622],\n",
       "       ...,\n",
       "       [-0.71968934, -1.1396013 ,  1.80045506, ..., -0.61239706,\n",
       "         1.2016    , -0.25003784],\n",
       "       [ 0.44285885, -1.08135426,  1.5238373 , ..., -1.31555701,\n",
       "         0.89208808,  1.18118328],\n",
       "       [ 1.74476904,  0.7548704 ,  0.10695152, ..., -0.96555013,\n",
       "        -1.6942205 ,  1.76143247]], shape=(128, 256))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = h5py.File('exercise8.h5', 'r')\n",
    "eeg_read = f['eeg_data'][:]\n",
    "f.close()\n",
    "eeg_read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Store a 2D Array of Spike Trains in a Group**  Store a 2D array of spike trains in a group called `recordings`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('exercise9.h5', 'w')\n",
    "f['recordings/spike_trains'] = np.random.randint(0, 2, (5, 20))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Read the Spike Trains**.  Read the `spike_trains` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = h5py.File('exercise9.h5', 'r')\n",
    "trains = f['recordings/spike_trains'][:]\n",
    "f.close()\n",
    "trains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Put Multiple Datasets into a Schema**:  Create a file with the following organization:\n",
    "\n",
    "  - behavior/\n",
    "    - trial: 10 values, uint8\n",
    "    - time: 100 values,  float32 \n",
    "    - lick_rate: 10 x 100 values, float32\n",
    "  - ephys/\n",
    "    - spike_times: 50 values, float64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Metadata in HDF5 as \"Attributes\"\n",
    "\n",
    "Labeling our data and keeping those labels together with the data itself is a key practice in having well-organized data, and hdf5 makes it easy to put labels directly inside the files.  \n",
    "\n",
    "One practice to be aware of:  \n",
    "  - General metadata for the file is often attached to the \"root\" group, so it's easy to find.  Things like experiment-level or session-level metadata are often stored there.  \n",
    "  - Metadata that describes a specific array are often attached to the hdf5 dataset itself.  Things like dimension labels, units, text descriptions of what the data represents can usually be found here.\n",
    "\n",
    ".\n",
    "\n",
    "| Code | Description |\n",
    "| :-- | :-- |\n",
    "| **`f.attrs['subject'] = 'Doug'`** | Add a \"subject\" attribute to the root-level group. |\n",
    "| **`f['x'].attrs['subject'] = 'Doug'`** | Add a \"subject\" attribute to the 'x' group or dataset. |\n",
    "| **`dict(f.attrs)`** | Get all the attributes attached to the root-level group as a dict.\n",
    "| **`dict(f['x'].attrs)`** | Get all the attributes attached to the 'x' group or dataset as a dict.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Create a file with the following schema:\n",
    "\n",
    "```\n",
    "meta_ex1.h5\n",
    "└─ / (root)\n",
    "   └─ subject_id = \"RatA\"  (attribute)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Reopen `meta_ex1.h5` and get the \"subject_id\" attribute value from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Create a file with the following schema:\n",
    "```\n",
    "meta_ex2.h5\n",
    "└─ / (root)\n",
    "   └─ lfp_data (dataset, shape=(100,))\n",
    "      └─ recording_date = \"2025-03-31\"  (attribute)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Reopen `meta_ex2.h5` and the `recording_date` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Create a file with the following schema:\n",
    "\n",
    "```\n",
    "meta_ex3.h5\n",
    "└─ / (root)\n",
    "   ├─ experimenter = \"Dr. Gray\"       (attribute)\n",
    "   ├─ experiment_type = \"Optogenetics\" (attribute)\n",
    "   └─ lab = \"NeuroLab\"                (attribute)\n",
    "   └─ spike_trains (dataset, shape=(4,50))\n",
    "      ├─ brain_region = \"Hippocampus\"   (attribute)\n",
    "      └─ num_channels = 4              (attribute)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Reopen `meta_ex3.h5` and get all the root-level metadata into a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling Dimensions of Datasets by Linking Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
